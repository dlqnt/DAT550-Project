{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: buzzfeed_partisan_data/all-partisan-sites.csv\n",
      "Downloaded: buzzfeed_partisan_data/pages-info.csv\n",
      "Downloaded: buzzfeed_partisan_data/domaintools-whois-results.csv\n",
      "Dataset shape: (677, 5)\n",
      "\n",
      "Columns: ['site', 'political_category', 'fb_id', 'unavailable_id', 'macedonian']\n",
      "\n",
      "First few rows:\n",
      "                                site political_category             fb_id  \\\n",
      "0                100percentfedup.com              right   311190048935167   \n",
      "1                21stcenturywire.com               left   182032255155419   \n",
      "2                     24dailynew.com              right   515629708825640   \n",
      "3                       24usnews.com              right  1430973860248840   \n",
      "4  4threvolutionarywar.wordpress.com               left               NaN   \n",
      "\n",
      "   unavailable_id  macedonian  \n",
      "0             NaN           0  \n",
      "1             NaN           0  \n",
      "2             NaN           1  \n",
      "3             NaN           1  \n",
      "4             NaN           0  \n",
      "\n",
      "Missing values per column:\n",
      "site                    0\n",
      "political_category      0\n",
      "fb_id                 118\n",
      "unavailable_id        608\n",
      "macedonian              0\n",
      "dtype: int64\n",
      "\n",
      "Facebook Pages Dataset shape: (452, 9)\n",
      "\n",
      "Facebook Pages columns: ['page_name', 'about', 'fan_count', 'talking_about_count', 'website', 'page_id', 'year', 'month', 'day']\n",
      "                                    page_name  \\\n",
      "0             Americans Against the Tea Party   \n",
      "1                                      act.tv   \n",
      "2                             New Blue United   \n",
      "3  Obama is the Worst President in US History   \n",
      "4                            RedFlag NewsDesk   \n",
      "\n",
      "                                               about  fan_count  \\\n",
      "0      We are your go to source for political news.      583256   \n",
      "1  Rise up and Resist! Your home for movement-ori...     285075   \n",
      "2                                                NaN    1476093   \n",
      "3                                                NaN    1569590   \n",
      "4  Daily headlines from the official RedFlagNews....       1533   \n",
      "\n",
      "   talking_about_count                 website          page_id    year  \\\n",
      "0                43343    http://www.aattp.org  108038612554992  2009.0   \n",
      "1               481748           http://act.tv  153418591515382     NaN   \n",
      "2                93116     www.bluetribune.com  188464111175168     NaN   \n",
      "3                41452                     NaN  296856040436954  2013.0   \n",
      "4                   76  http://redflagnews.com  492836854251934  2016.0   \n",
      "\n",
      "   month   day  \n",
      "0    4.0   3.0  \n",
      "1    NaN   NaN  \n",
      "2    NaN   NaN  \n",
      "3    1.0   1.0  \n",
      "4    3.0  16.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "# Create a directory for the data\n",
    "data_dir = \"buzzfeed_partisan_data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Function to download files directly instead of using git\n",
    "def download_github_file(repo_owner, repo_name, path, save_path):\n",
    "    \"\"\"Download a specific file from GitHub\"\"\"\n",
    "    url = f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/master/{path}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {save_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download: {url}\")\n",
    "        return False\n",
    "\n",
    "# Download the main CSV files\n",
    "files_to_download = [\n",
    "    \"data/all-partisan-sites.csv\",\n",
    "    \"data/pages-info.csv\",\n",
    "    \"data/domaintools-whois-results.csv\"\n",
    "]\n",
    "\n",
    "for file_path in files_to_download:\n",
    "    save_path = os.path.join(data_dir, os.path.basename(file_path))\n",
    "    download_github_file(\n",
    "        \"BuzzFeedNews\", \n",
    "        \"2017-08-partisan-sites-and-facebook-pages\", \n",
    "        file_path, \n",
    "        save_path\n",
    "    )\n",
    "\n",
    "# Load the partisan sites data\n",
    "partisan_sites_path = os.path.join(data_dir, \"all-partisan-sites.csv\")\n",
    "if os.path.exists(partisan_sites_path):\n",
    "    partisan_sites = pd.read_csv(partisan_sites_path)\n",
    "    \n",
    "    # Look at the structure\n",
    "    print(\"Dataset shape:\", partisan_sites.shape)\n",
    "    print(\"\\nColumns:\", partisan_sites.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(partisan_sites.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(partisan_sites.isnull().sum())\n",
    "    \n",
    "    # Explore partisan distribution\n",
    "    if 'partisanship' in partisan_sites.columns:\n",
    "        print(\"\\nPartisan distribution:\")\n",
    "        print(partisan_sites['partisanship'].value_counts())\n",
    "    elif 'partisan_code' in partisan_sites.columns:\n",
    "        print(\"\\nPartisan distribution:\")\n",
    "        print(partisan_sites['partisan_code'].value_counts())\n",
    "else:\n",
    "    print(\"Failed to download the partisan sites dataset\")\n",
    "\n",
    "# Load Facebook page info\n",
    "pages_info_path = os.path.join(data_dir, \"pages-info.csv\")\n",
    "if os.path.exists(pages_info_path):\n",
    "    pages_info = pd.read_csv(pages_info_path)\n",
    "    print(\"\\nFacebook Pages Dataset shape:\", pages_info.shape)\n",
    "    print(\"\\nFacebook Pages columns:\", pages_info.columns.tolist())\n",
    "    print(pages_info.head())\n",
    "else:\n",
    "    print(\"Failed to download the pages info dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Political category distribution:\n",
      "political_category\n",
      "right    499\n",
      "left     178\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Macedonian sites distribution:\n",
      "macedonian\n",
      "0    596\n",
      "1     81\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Macedonian sites by political category:\n",
      "macedonian            0   1\n",
      "political_category         \n",
      "left                176   2\n",
      "right               420  79\n"
     ]
    }
   ],
   "source": [
    "# Examine distribution of political categories\n",
    "print(\"\\nPolitical category distribution:\")\n",
    "political_dist = partisan_sites['political_category'].value_counts()\n",
    "print(political_dist)\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='political_category', data=partisan_sites)\n",
    "plt.title('Distribution of Partisan Sites by Political Category')\n",
    "plt.savefig(os.path.join(data_dir, 'political_distribution.png'))\n",
    "plt.close()\n",
    "\n",
    "# Check Macedonian sites\n",
    "print(\"\\nMacedonian sites distribution:\")\n",
    "macedonian_dist = partisan_sites['macedonian'].value_counts()\n",
    "print(macedonian_dist)\n",
    "print(\"\\nMacedonian sites by political category:\")\n",
    "print(pd.crosstab(partisan_sites['political_category'], partisan_sites['macedonian']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb_id dtype: object\n",
      "page_id dtype: int64\n",
      "\n",
      "Number of sites with matched Facebook pages: 490\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(\"fb_id dtype:\", partisan_sites['fb_id'].dtype)\n",
    "print(\"page_id dtype:\", pages_info['page_id'].dtype)\n",
    "\n",
    "# Convert both columns to strings for proper merging\n",
    "partisan_sites['fb_id'] = partisan_sites['fb_id'].astype(str)\n",
    "pages_info['page_id'] = pages_info['page_id'].astype(str)\n",
    "\n",
    "# Now merge will work\n",
    "merged_data = pd.merge(\n",
    "    partisan_sites, \n",
    "    pages_info,\n",
    "    left_on='fb_id', \n",
    "    right_on='page_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of sites with matched Facebook pages: {len(merged_data)}\")\n",
    "\n",
    "# Compare fan counts by political category\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='political_category', y='fan_count', data=merged_data)\n",
    "plt.yscale('log')\n",
    "plt.title('Facebook Fan Count by Political Category')\n",
    "plt.savefig(os.path.join(data_dir, 'fan_count_by_category.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WHOIS data shape: (663, 70)\n",
      "\n",
      "WHOIS columns: ['domain', 'whois url', 'admin contact name', 'admin contact org', 'admin contact street', 'admin contact city', 'admin contact state', 'admin contact postal', 'admin contact country', 'admin contact phone', 'admin contact fax', 'admin contact email 1', 'admin contact email 2', 'admin contact email 3', 'billing contact name', 'billing contact org', 'billing contact street', 'billing contact city', 'billing contact state', 'billing contact postal', 'billing contact country', 'billing contact phone', 'billing contact fax', 'billing contact email 1', 'billing contact email 2', 'billing contact email 3', 'registrant contact name', 'registrant contact org', 'registrant contact street', 'registrant contact city', 'registrant contact state', 'registrant contact postal', 'registrant contact country', 'registrant contact phone', 'registrant contact fax', 'registrant contact email 1', 'registrant contact email 2', 'registrant contact email 3', 'technical contact name', 'technical contact org', 'technical contact street', 'technical contact city', 'technical contact state', 'technical contact postal', 'technical contact country', 'technical contact phone', 'technical contact fax', 'technical contact email 1', 'technical contact email 2', 'technical contact email 3', 'create date', 'expiration date', 'additional whois email 1', 'additional whois email 2', 'additional whois email 3', 'name server 1 - host', 'name server 2 - host', 'name server 3 - host', 'name server 4 - host', 'name server 5 - host', 'name server 6 - host', 'name server 7 - host', 'name server 8 - host', 'registrar', 'registrar status 1', 'registrar status 2', 'registrar status 3', 'registrar status 4', 'registrar status 5', 'registrar status 6']\n",
      "\n",
      "Sample WHOIS data:\n",
      "                domain                                          whois url  \\\n",
      "0  100percentfedup.com  https://whois.domaintools.com/100percentfedup.com   \n",
      "1  21stcenturywire.com  https://whois.domaintools.com/21stcenturywire.com   \n",
      "2       24dailynew.com       https://whois.domaintools.com/24dailynew.com   \n",
      "3         24usnews.com         https://whois.domaintools.com/24usnews.com   \n",
      "4            63red.com            https://whois.domaintools.com/63red.com   \n",
      "\n",
      "           admin contact name  \\\n",
      "0        Registration Private   \n",
      "1               P. Henningsen   \n",
      "2  Domain Admin, C/O ID#10760   \n",
      "3          Aleksandar Nikolov   \n",
      "4                 WHOIS AGENT   \n",
      "\n",
      "                                   admin contact org  \\\n",
      "0                              Domains By Proxy, LLC   \n",
      "1                                                NaN   \n",
      "2  Privacy Protection Service INC d/b/a PrivacyPr...   \n",
      "3                                           24usnews   \n",
      "4              WHOIS PRIVACY PROTECTION SERVICE, INC   \n",
      "\n",
      "                      admin contact street admin contact city  \\\n",
      "0  DomainsByProxy.com,14455 N. Hayden Road         Scottsdale   \n",
      "1            Unit 220, 8-10 Sunnyhill Road             London   \n",
      "2                                PO Box 16        Nobby Beach   \n",
      "3                        Georgi Dimitrov 7              Veles   \n",
      "4                 PO BOX 639,C/O 63RED.COM           KIRKLAND   \n",
      "\n",
      "  admin contact state admin contact postal admin contact country  \\\n",
      "0             Arizona                85260                    us   \n",
      "1                 NaN             SW16 2BJ                    gb   \n",
      "2          Queensland             QLD 4218                    au   \n",
      "3                  XX                 1400                    mk   \n",
      "4                  WA                98083                    us   \n",
      "\n",
      "   admin contact phone  ...  name server 6 - host name server 7 - host  \\\n",
      "0         1.480624e+10  ...                   NaN                  NaN   \n",
      "1         4.475470e+11  ...                   NaN                  NaN   \n",
      "2         4.536947e+09  ...                   NaN                  NaN   \n",
      "3         3.897625e+10  ...                   NaN                  NaN   \n",
      "4         1.425274e+10  ...                   NaN                  NaN   \n",
      "\n",
      "   name server 8 - host                                registrar  \\\n",
      "0                   NaN                    GO DADDY SOFTWARE INC   \n",
      "1                   NaN                    GO DADDY SOFTWARE INC   \n",
      "2                   NaN  PDR LTD. D/B/A PUBLICDOMAINREGISTRY.COM   \n",
      "3                   NaN                       TUCOWS DOMAINS INC   \n",
      "4                   NaN                                ENOM, INC   \n",
      "\n",
      "         registrar status 1 registrar status 2 registrar status 3  \\\n",
      "0  clientTransferProhibited                NaN                NaN   \n",
      "1  clientTransferProhibited                NaN                NaN   \n",
      "2  clientTransferProhibited                NaN                NaN   \n",
      "3  clientTransferProhibited                NaN                NaN   \n",
      "4  clientTransferProhibited                NaN                NaN   \n",
      "\n",
      "  registrar status 4 registrar status 5 registrar status 6  \n",
      "0                NaN                NaN                NaN  \n",
      "1                NaN                NaN                NaN  \n",
      "2                NaN                NaN                NaN  \n",
      "3                NaN                NaN                NaN  \n",
      "4                NaN                NaN                NaN  \n",
      "\n",
      "[5 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "# Analyze domain registration data\n",
    "whois_path = os.path.join(data_dir, \"domaintools-whois-results.csv\")\n",
    "if os.path.exists(whois_path):\n",
    "    whois_data = pd.read_csv(whois_path)\n",
    "    print(\"\\nWHOIS data shape:\", whois_data.shape)\n",
    "    print(\"\\nWHOIS columns:\", whois_data.columns.tolist())\n",
    "    \n",
    "    # Check sample data\n",
    "    print(\"\\nSample WHOIS data:\")\n",
    "    print(whois_data.head())\n",
    "    \n",
    "    # Check registration dates if available\n",
    "    if 'create_date' in whois_data.columns:\n",
    "        # Convert to datetime\n",
    "        whois_data['create_date'] = pd.to_datetime(whois_data['create_date'], errors='coerce')\n",
    "        \n",
    "        # Extract year and month\n",
    "        whois_data['reg_year'] = whois_data['create_date'].dt.year\n",
    "        \n",
    "        # Plot registration years\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        whois_data['reg_year'].value_counts().sort_index().plot(kind='bar')\n",
    "        plt.title('Domain Registration Years')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Number of Domains')\n",
    "        plt.savefig(os.path.join(data_dir, 'domain_reg_years.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "About text statistics:\n",
      "Number of pages with descriptions: 426\n",
      "Average length: 94.4 characters\n",
      "Average word count: 13.9 words\n",
      "\n",
      "Top 20 most common words in page descriptions:\n",
      "the: 343\n",
      "and: 241\n",
      "news: 165\n",
      "to: 152\n",
      "of: 148\n",
      "a: 118\n",
      "com: 111\n",
      "is: 111\n",
      "for: 82\n",
      "on: 70\n",
      "http: 68\n",
      "we: 67\n",
      "in: 55\n",
      "www: 55\n",
      "conservative: 52\n",
      "media: 51\n",
      "political: 48\n",
      "our: 48\n",
      "s: 48\n",
      "that: 45\n"
     ]
    }
   ],
   "source": [
    "# Check for any text content in our datasets\n",
    "if 'about' in pages_info.columns:\n",
    "    # Filter out missing about descriptions\n",
    "    about_text = pages_info['about'].dropna()\n",
    "    \n",
    "    # Basic text statistics\n",
    "    text_lengths = about_text.str.len()\n",
    "    word_counts = about_text.str.split().str.len()\n",
    "    \n",
    "    print(\"\\nAbout text statistics:\")\n",
    "    print(f\"Number of pages with descriptions: {len(about_text)}\")\n",
    "    print(f\"Average length: {text_lengths.mean():.1f} characters\")\n",
    "    print(f\"Average word count: {word_counts.mean():.1f} words\")\n",
    "    \n",
    "    # Word frequency analysis\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    \n",
    "    # Combine all text\n",
    "    all_text = ' '.join(about_text)\n",
    "    \n",
    "    # Simple tokenization (you might want to use NLTK or spaCy for better tokenization)\n",
    "    words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Top 20 most common words\n",
    "    print(\"\\nTop 20 most common words in page descriptions:\")\n",
    "    for word, count in word_freq.most_common(20):\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sites with multiple Facebook pages:\n",
      "site\n",
      "topsecretinfodump.com    2\n",
      "cscmediagroupus.com      2\n",
      "usanewshome.com          2\n",
      "donaldtrumpnews.co       2\n",
      "westernjournalism.com    2\n",
      "thetruthdivision.com     2\n",
      "cosmo-politics.com       2\n",
      "redflagnews.com          2\n",
      "analogpolitics.com       2\n",
      "bluedotdaily.com         2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example - Facebook pages for topsecretinfodump.com:\n",
      "                      site political_category            fb_id  \\\n",
      "568  topsecretinfodump.com              right  507165509486196   \n",
      "569  topsecretinfodump.com              right  592074077639509   \n",
      "\n",
      "     unavailable_id  macedonian  \n",
      "568             NaN           0  \n",
      "569             NaN           0  \n",
      "\n",
      "Exploration Summary:\n",
      "Total partisan sites: 677\n",
      "  - With Facebook IDs: 677\n",
      "  - Macedonian sites: 81\n",
      "Total Facebook pages: 452\n",
      "Matched sites with pages: 490\n"
     ]
    }
   ],
   "source": [
    "# Check for domains with multiple Facebook pages\n",
    "duplicate_sites = partisan_sites['site'].value_counts()\n",
    "sites_with_multiple_pages = duplicate_sites[duplicate_sites > 1]\n",
    "\n",
    "if len(sites_with_multiple_pages) > 0:\n",
    "    print(\"\\nSites with multiple Facebook pages:\")\n",
    "    print(sites_with_multiple_pages)\n",
    "    \n",
    "    # Example of a site with multiple pages\n",
    "    example_site = sites_with_multiple_pages.index[0]\n",
    "    print(f\"\\nExample - Facebook pages for {example_site}:\")\n",
    "    print(partisan_sites[partisan_sites['site'] == example_site])\n",
    "\n",
    "# Summary statistics for exploration\n",
    "print(\"\\nExploration Summary:\")\n",
    "print(f\"Total partisan sites: {len(partisan_sites)}\")\n",
    "print(f\"  - With Facebook IDs: {partisan_sites['fb_id'].notna().sum()}\")\n",
    "print(f\"  - Macedonian sites: {partisan_sites['macedonian'].sum()}\")\n",
    "print(f\"Total Facebook pages: {len(pages_info)}\")\n",
    "print(f\"Matched sites with pages: {len(merged_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-CPv1Fx27-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
