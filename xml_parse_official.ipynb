{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml_parse.ipynb\n",
    "# Purpose: Parse the official SemEval train/test XML data, extract raw text\n",
    "# and publisher info, and save to separate CSV files for BERT input.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from tqdm import tqdm # Optional: for progress bar\n",
    "\n",
    "# --- Configuration ---\n",
    "# *** IMPORTANT: Adjust these paths to match where semeval_download.ipynb extracted the files ***\n",
    "data_dir = \"hyperpartisan_data_official\" \n",
    "train_articles_dir = os.path.join(data_dir, \"articles-training-byarticle-20181122\")\n",
    "train_gt_path = os.path.join(data_dir, \"ground-truth-training-byarticle-20181122.xml\") \n",
    "test_articles_dir = os.path.join(data_dir, \"articles-test-byarticle-20181207\") \n",
    "test_gt_path = os.path.join(data_dir, \"ground-truth-test-byarticle-20181207.xml\") \n",
    "\n",
    "output_train_csv = os.path.join(data_dir, \"official_train_data.csv\")\n",
    "output_test_csv = os.path.join(data_dir, \"official_test_data.csv\")\n",
    "\n",
    "# --- Parsing Functions ---\n",
    "def parse_ground_truth(xml_path):\n",
    "    \"\"\"Parses the ground truth XML file and returns dicts mapping ID to label and publisher.\"\"\"\n",
    "    print(f\"Parsing ground truth: {xml_path}\")\n",
    "    if not os.path.exists(xml_path):\n",
    "        raise FileNotFoundError(f\"Ground truth file not found: {xml_path}\")\n",
    "    \n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    labels = {}\n",
    "    publishers = {} \n",
    "    articles_in_gt = 0\n",
    "    for article in root.findall('.//article'):\n",
    "        articles_in_gt += 1\n",
    "        article_id = article.get('id')\n",
    "        hyperpartisan_attr = article.get('hyperpartisan')\n",
    "        publisher = article.get('portal', None) # Get portal attribute\n",
    "\n",
    "        if article_id is None:\n",
    "            print(f\"Warning: Found article tag without 'id' in {xml_path}. Skipping.\")\n",
    "            continue\n",
    "        if hyperpartisan_attr is None:\n",
    "             print(f\"Warning: Article {article_id} missing 'hyperpartisan' attribute in {xml_path}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        labels[article_id] = hyperpartisan_attr.lower() == 'true'\n",
    "        \n",
    "        # Store publisher if found, otherwise mark for later extraction\n",
    "        publishers[article_id] = publisher if publisher else \"extract_from_filename\"\n",
    "\n",
    "    print(f\"Parsed {articles_in_gt} entries from ground truth. Found {len(labels)} valid article IDs with labels.\")\n",
    "    return labels, publishers\n",
    "\n",
    "def parse_articles_directory(dir_path, ground_truth_labels, ground_truth_publishers):\n",
    "    \"\"\"Parses a directory of article XMLs, extracts raw text, publisher, and adds labels.\"\"\"\n",
    "    print(f\"Parsing articles directory: {dir_path}\")\n",
    "    if not os.path.isdir(dir_path):\n",
    "         raise FileNotFoundError(f\"Articles directory not found: {dir_path}\")\n",
    "    \n",
    "    articles_data = []\n",
    "    filenames = [f for f in os.listdir(dir_path) if f.endswith('.xml')]\n",
    "    print(f\"Found {len(filenames)} article XML files.\")\n",
    "\n",
    "    skipped_count = 0\n",
    "    parsed_count = 0\n",
    "\n",
    "    for filename in tqdm(filenames, desc=f\"Parsing {os.path.basename(dir_path)}\"):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        try:\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot() # Assumes the root element is <article>\n",
    "\n",
    "            article_id = root.get('id')\n",
    "            if not article_id: # Check if ID exists in article XML\n",
    "                print(f\"Warning: ID attribute missing in file {filename}. Trying to extract from filename...\")\n",
    "                # Attempt to extract ID from filename (e.g., article12345.xml)\n",
    "                id_match = re.match(r'article(\\d+)', filename)\n",
    "                if id_match:\n",
    "                    article_id = id_match.group(1)\n",
    "                    print(f\"  Extracted ID '{article_id}' from filename.\")\n",
    "                else:\n",
    "                    print(f\"  Could not extract ID from filename {filename}. Skipping.\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "            if article_id not in ground_truth_labels:\n",
    "                print(f\"Warning: Article ID '{article_id}' from file {filename} not found in ground truth. Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            title = root.get('title', '') # Use default if missing\n",
    "\n",
    "            # Extract text content - joining paragraphs is usually best\n",
    "            paragraphs = root.findall('.//p')\n",
    "            if paragraphs:\n",
    "                # Join text content of all <p> tags, handle None text\n",
    "                text = \"\\n\".join(p.text.strip() for p in paragraphs if p.text).strip()\n",
    "            else: \n",
    "                 # Fallback: get all text within the article tag, might include unwanted stuff\n",
    "                 text = \"\".join(node.strip() for node in root.itertext() if node and node.strip()).strip()\n",
    "                 if not text:\n",
    "                      print(f\"Warning: No text found in article {article_id} ({filename}).\")\n",
    "\n",
    "\n",
    "            # --- Determine Publisher ---\n",
    "            publisher = ground_truth_publishers.get(article_id, \"unknown\")\n",
    "            if publisher == \"extract_from_filename\" or publisher == \"unknown\":\n",
    "                 # Try extracting from filename (pattern: article<ID>_<PUBLISHER>.xml)\n",
    "                 match = re.match(r'article\\d+_([a-zA-Z0-9-]+)\\.xml', filename)\n",
    "                 if match:\n",
    "                      publisher = match.group(1)\n",
    "                 else:\n",
    "                      publisher = \"unknown\" # Still unknown if pattern fails\n",
    "                 # print(f\"  Publisher for {article_id} from filename: {publisher}\") # Optional debug\n",
    "            # ---\n",
    "\n",
    "            articles_data.append({\n",
    "                'id': article_id,\n",
    "                'title': title if title else 'No Title', # Handle potentially missing titles\n",
    "                'text': text, # Use raw text, ensure it's not None\n",
    "                'publisher': publisher, \n",
    "                'hyperpartisan': ground_truth_labels[article_id]\n",
    "            })\n",
    "            parsed_count += 1\n",
    "\n",
    "        except ET.ParseError:\n",
    "            print(f\"Warning: Skipping file {filename} due to XML parse error.\")\n",
    "            skipped_count += 1\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Skipping file {filename} due to unexpected error: {e}\")\n",
    "             skipped_count += 1\n",
    "\n",
    "    df = pd.DataFrame(articles_data)\n",
    "    # Simple check for empty text - replace with a placeholder if needed, or drop\n",
    "    empty_text_count = (df['text'] == \"\").sum()\n",
    "    if empty_text_count > 0:\n",
    "        print(f\"Warning: Found {empty_text_count} articles with empty text after parsing.\")\n",
    "        # df['text'] = df['text'].replace(\"\", \"[NO TEXT EXTRACTED]\") # Option: Replace\n",
    "        # df = df[df['text'] != \"\"] # Option: Drop\n",
    "\n",
    "    print(f\"Successfully parsed {parsed_count} articles. Skipped {skipped_count} files.\")\n",
    "    return df\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# --- Process Training Data ---\n",
    "print(\"\\n===== PROCESSING OFFICIAL TRAINING DATA =====\")\n",
    "try:\n",
    "    train_labels, train_publishers = parse_ground_truth(train_gt_path)\n",
    "    train_df = parse_articles_directory(train_articles_dir, train_labels, train_publishers)\n",
    "    if not train_df.empty:\n",
    "        print(\"\\nTraining Data Info:\")\n",
    "        print(train_df.info())\n",
    "        print(\"\\nTraining Label Distribution:\")\n",
    "        print(train_df['hyperpartisan'].value_counts(normalize=True))\n",
    "        print(\"\\nTraining Publisher Distribution (Top 10):\")\n",
    "        print(train_df['publisher'].value_counts().head(10))\n",
    "        # Save training data\n",
    "        train_df.to_csv(output_train_csv, index=False)\n",
    "        print(f\"\\nSaved official training data to {output_train_csv}\")\n",
    "    else:\n",
    "        print(\"Failed to create training DataFrame.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error processing training data: {e}\")\n",
    "    train_df = pd.DataFrame() \n",
    "\n",
    "# --- Process Test Data ---\n",
    "print(\"\\n===== PROCESSING OFFICIAL TEST DATA =====\")\n",
    "try:\n",
    "    test_labels, test_publishers = parse_ground_truth(test_gt_path)\n",
    "    test_df = parse_articles_directory(test_articles_dir, test_labels, test_publishers)\n",
    "    if not test_df.empty:\n",
    "        print(\"\\nTest Data Info:\")\n",
    "        print(test_df.info())\n",
    "        print(\"\\nTest Label Distribution:\")\n",
    "        print(test_df['hyperpartisan'].value_counts(normalize=True))\n",
    "        print(\"\\nTest Publisher Distribution (Top 10):\")\n",
    "        print(test_df['publisher'].value_counts().head(10))\n",
    "         # Save test data\n",
    "        test_df.to_csv(output_test_csv, index=False)\n",
    "        print(f\"\\nSaved official test data to {output_test_csv}\")\n",
    "    else:\n",
    "         print(\"Failed to create test DataFrame.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error processing test data: {e}\")\n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "print(\"\\nXML parsing and CSV creation complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
