{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PROCESSING OFFICIAL TRAINING DATA =====\n",
      "Parsing ground truth: hyperpartisan_data_official/ground-truth-training-byarticle-20181122.xml\n",
      "Parsed 645 entries from ground truth. Found 645 valid article IDs with labels.\n",
      "Parsing articles XML file: hyperpartisan_data_official/articles-training-byarticle-20181122.xml\n",
      "Found 645 article tags in articles-training-byarticle-20181122.xml. Parsing...\n",
      "Warning: Found 2 articles with empty text after parsing.\n",
      "Successfully parsed 645 articles. Skipped 0 entries.\n",
      "\n",
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 645 entries, 0 to 644\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             645 non-null    object\n",
      " 1   title          645 non-null    object\n",
      " 2   text           645 non-null    object\n",
      " 3   publisher      645 non-null    object\n",
      " 4   hyperpartisan  645 non-null    bool  \n",
      "dtypes: bool(1), object(4)\n",
      "memory usage: 3.2 MB\n",
      "\n",
      "Training Label Distribution:\n",
      "hyperpartisan\n",
      "False    0.631008\n",
      "True     0.368992\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training Publisher Distribution (Top 10):\n",
      "publisher\n",
      "unknown    645\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved official training data to hyperpartisan_data_official/official_train_data.csv\n",
      "\n",
      "===== PROCESSING OFFICIAL TEST DATA =====\n",
      "Parsing ground truth: hyperpartisan_data_official/ground-truth-test-byarticle-20181207.xml\n",
      "Parsed 628 entries from ground truth. Found 628 valid article IDs with labels.\n",
      "Parsing articles XML file: hyperpartisan_data_official/articles-test-byarticle-20181207.xml\n",
      "Found 628 article tags in articles-test-byarticle-20181207.xml. Parsing...\n",
      "Warning: Found 1 articles with empty text after parsing.\n",
      "Successfully parsed 628 articles. Skipped 0 entries.\n",
      "\n",
      "Test Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 628 entries, 0 to 627\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             628 non-null    object\n",
      " 1   title          628 non-null    object\n",
      " 2   text           628 non-null    object\n",
      " 3   publisher      628 non-null    object\n",
      " 4   hyperpartisan  628 non-null    bool  \n",
      "dtypes: bool(1), object(4)\n",
      "memory usage: 3.4 MB\n",
      "\n",
      "Test Label Distribution:\n",
      "hyperpartisan\n",
      "False    0.5\n",
      "True     0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test Publisher Distribution (Top 10):\n",
      "publisher\n",
      "unknown    628\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved official test data to hyperpartisan_data_official/official_test_data.csv\n",
      "\n",
      "XML parsing and CSV creation complete.\n"
     ]
    }
   ],
   "source": [
    "# xml_parse.ipynb (No TQDM)\n",
    "# Purpose: Parse the official SemEval train/test XML data files (single large XML format), \n",
    "# extract raw text and publisher info, and save to separate CSV files for BERT input.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "# Removed: from tqdm import tqdm \n",
    "\n",
    "# --- Configuration ---\n",
    "# *** IMPORTANT: Adjust these paths to match where semeval_download.ipynb extracted the files ***\n",
    "data_dir = \"hyperpartisan_data_official\" \n",
    "train_articles_xml_path = os.path.join(data_dir, \"articles-training-byarticle-20181122.xml\")\n",
    "train_gt_path = os.path.join(data_dir, \"ground-truth-training-byarticle-20181122.xml\") \n",
    "test_articles_xml_path = os.path.join(data_dir, \"articles-test-byarticle-20181207.xml\") \n",
    "test_gt_path = os.path.join(data_dir, \"ground-truth-test-byarticle-20181207.xml\") \n",
    "\n",
    "output_train_csv = os.path.join(data_dir, \"official_train_data.csv\")\n",
    "output_test_csv = os.path.join(data_dir, \"official_test_data.csv\")\n",
    "\n",
    "# --- Parsing Functions ---\n",
    "def parse_ground_truth(xml_path):\n",
    "    \"\"\"Parses the ground truth XML file and returns dicts mapping ID to label and publisher.\"\"\"\n",
    "    print(f\"Parsing ground truth: {xml_path}\")\n",
    "    if not os.path.exists(xml_path):\n",
    "        raise FileNotFoundError(f\"Ground truth file not found: {xml_path}\")\n",
    "    \n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    labels = {}\n",
    "    publishers = {} \n",
    "    articles_in_gt = 0\n",
    "    for article in root.findall('.//article'): # Find all article tags within the ground truth\n",
    "        articles_in_gt += 1\n",
    "        article_id = article.get('id')\n",
    "        hyperpartisan_attr = article.get('hyperpartisan')\n",
    "        # ---> Get publisher directly from the ground truth file's 'portal' attribute <---\n",
    "        publisher = article.get('portal', \"unknown\") # Use 'unknown' if missing\n",
    "\n",
    "        if article_id is None:\n",
    "            print(f\"Warning: Found article tag without 'id' in {xml_path}. Skipping.\")\n",
    "            continue\n",
    "        if hyperpartisan_attr is None:\n",
    "             print(f\"Warning: Article {article_id} missing 'hyperpartisan' attribute in {xml_path}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        labels[article_id] = hyperpartisan_attr.lower() == 'true'\n",
    "        publishers[article_id] = publisher\n",
    "\n",
    "    print(f\"Parsed {articles_in_gt} entries from ground truth. Found {len(labels)} valid article IDs with labels.\")\n",
    "    return labels, publishers\n",
    "\n",
    "def parse_single_articles_xml(xml_path, ground_truth_labels, ground_truth_publishers):\n",
    "    \"\"\"Parses a single large XML file containing multiple articles.\"\"\"\n",
    "    print(f\"Parsing articles XML file: {xml_path}\")\n",
    "    if not os.path.exists(xml_path):\n",
    "         raise FileNotFoundError(f\"Articles XML file not found: {xml_path}\")\n",
    "    \n",
    "    articles_data = []\n",
    "    skipped_count = 0\n",
    "    parsed_count = 0\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot() # Should be the root element (e.g., <articles>)\n",
    "\n",
    "        # Iterate through each <article> tag within the root\n",
    "        # Removed tqdm wrapper from the loop below\n",
    "        all_articles = root.findall('.//article')\n",
    "        total_articles_in_file = len(all_articles)\n",
    "        print(f\"Found {total_articles_in_file} article tags in {os.path.basename(xml_path)}. Parsing...\")\n",
    "\n",
    "        for i, article_element in enumerate(all_articles):\n",
    "            # Optional: Print progress every N articles\n",
    "            # if (i + 1) % 100 == 0:\n",
    "            #     print(f\"  Processed {i+1}/{total_articles_in_file} articles...\")\n",
    "                \n",
    "            article_id = article_element.get('id')\n",
    "            if not article_id:\n",
    "                print(f\"Warning: Found article tag without 'id' in {xml_path} (entry ~{i+1}). Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            if article_id not in ground_truth_labels:\n",
    "                print(f\"Warning: Article ID '{article_id}' not found in ground truth. Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            title = article_element.get('title', '') \n",
    "\n",
    "            # Extract text content\n",
    "            paragraphs = article_element.findall('.//p')\n",
    "            if paragraphs:\n",
    "                text = \"\\n\".join(p.text.strip() for p in paragraphs if p.text).strip()\n",
    "            else: \n",
    "                 text = \"\".join(node.strip() for node in article_element.itertext() if node and node.strip()).strip()\n",
    "                 if not text:\n",
    "                      print(f\"Warning: No text found in article {article_id} in {xml_path}.\")\n",
    "\n",
    "            # Get publisher from the ground truth data (already parsed)\n",
    "            publisher = ground_truth_publishers.get(article_id, \"unknown\") \n",
    "\n",
    "            articles_data.append({\n",
    "                'id': article_id,\n",
    "                'title': title if title else 'No Title', \n",
    "                'text': text, \n",
    "                'publisher': publisher, \n",
    "                'hyperpartisan': ground_truth_labels[article_id]\n",
    "            })\n",
    "            parsed_count += 1\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"FATAL ERROR: Could not parse XML file {xml_path}: {e}\")\n",
    "        return pd.DataFrame() # Return empty dataframe on critical parse error\n",
    "    except Exception as e:\n",
    "         print(f\"FATAL ERROR: Unexpected error parsing {xml_path}: {e}\")\n",
    "         return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(articles_data)\n",
    "    empty_text_count = (df['text'] == \"\").sum()\n",
    "    if empty_text_count > 0:\n",
    "        print(f\"Warning: Found {empty_text_count} articles with empty text after parsing.\")\n",
    "        \n",
    "    print(f\"Successfully parsed {parsed_count} articles. Skipped {skipped_count} entries.\")\n",
    "    return df\n",
    "# --- End New Function ---\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# --- Process Training Data ---\n",
    "print(\"\\n===== PROCESSING OFFICIAL TRAINING DATA =====\")\n",
    "try:\n",
    "    train_labels, train_publishers = parse_ground_truth(train_gt_path)\n",
    "    train_df = parse_single_articles_xml(train_articles_xml_path, train_labels, train_publishers)\n",
    "    if not train_df.empty:\n",
    "        print(\"\\nTraining Data Info:\")\n",
    "        train_df.info(memory_usage='deep') # Show memory usage too\n",
    "        print(\"\\nTraining Label Distribution:\")\n",
    "        print(train_df['hyperpartisan'].value_counts(normalize=True))\n",
    "        print(\"\\nTraining Publisher Distribution (Top 10):\")\n",
    "        print(train_df['publisher'].value_counts().head(10))\n",
    "        # Save training data\n",
    "        train_df.to_csv(output_train_csv, index=False)\n",
    "        print(f\"\\nSaved official training data to {output_train_csv}\")\n",
    "    else:\n",
    "        print(\"Failed to create training DataFrame.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error processing training data: {e}\")\n",
    "    train_df = pd.DataFrame() \n",
    "\n",
    "# --- Process Test Data ---\n",
    "print(\"\\n===== PROCESSING OFFICIAL TEST DATA =====\")\n",
    "try:\n",
    "    test_labels, test_publishers = parse_ground_truth(test_gt_path)\n",
    "    test_df = parse_single_articles_xml(test_articles_xml_path, test_labels, test_publishers)\n",
    "    if not test_df.empty:\n",
    "        print(\"\\nTest Data Info:\")\n",
    "        test_df.info(memory_usage='deep') # Show memory usage\n",
    "        print(\"\\nTest Label Distribution:\")\n",
    "        print(test_df['hyperpartisan'].value_counts(normalize=True))\n",
    "        print(\"\\nTest Publisher Distribution (Top 10):\")\n",
    "        print(test_df['publisher'].value_counts().head(10))\n",
    "         # Save test data\n",
    "        test_df.to_csv(output_test_csv, index=False)\n",
    "        print(f\"\\nSaved official test data to {output_test_csv}\")\n",
    "    else:\n",
    "         print(\"Failed to create test DataFrame.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error processing test data: {e}\")\n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "print(\"\\nXML parsing and CSV creation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
