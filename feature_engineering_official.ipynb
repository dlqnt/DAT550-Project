{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fl/_zgwb4_x0653c4k5cg1l1mxr0000gn/T/ipykernel_33319/2222226020.py:11: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Official Train/Test Data ---\n",
      "Loaded Training Data: (645, 5)\n",
      "Loaded Test Data: (628, 5)\n",
      "--- Starting Feature Engineering Process ---\n",
      "\n",
      "Processing Training Set Features...\n",
      "Extracting length-based features...\n",
      "Extracting lexical features...\n",
      "Extracting sentiment features...\n",
      "\n",
      "Processing Test Set Features...\n",
      "Extracting length-based features...\n",
      "Extracting lexical features...\n",
      "Extracting sentiment features...\n",
      "\n",
      "Extracting TF-IDF Features (Fit on Train, Transform Train & Test)...\n",
      "Created 1000 TF-IDF features\n",
      "Saved TF-IDF vectorizer to hyperpartisan_features_official/tfidf_vectorizer.pkl\n",
      "\n",
      "Extracting N-gram Features (Fit on Train, Transform Train & Test)...\n",
      "Created 100 bigram features\n",
      "Saved Bigram vectorizer to hyperpartisan_features_official/bigram_vectorizer.pkl\n",
      "\n",
      "Combining all features...\n",
      "Final Training Feature Matrix shape: (645, 1137)\n",
      "Final Test Feature Matrix shape: (628, 1137)\n",
      "\n",
      "Scaling numerical features...\n",
      "Found 1137 numerical features to scale.\n",
      "Features scaled.\n",
      "\n",
      "Saving scaled features and labels...\n",
      "Saved Scaler to hyperpartisan_features_official/scaler.pkl\n",
      "Saved training features to hyperpartisan_features_official/X_train_scaled.csv\n",
      "Saved test features to hyperpartisan_features_official/X_test_scaled.csv\n",
      "Saved training labels to hyperpartisan_features_official/y_train.csv\n",
      "Saved test labels to hyperpartisan_features_official/y_test.csv\n",
      "\n",
      "--- Feature Engineering Summary ---\n",
      "Total features generated: 1137\n",
      "Length features: 9\n",
      "Lexical features: 22\n",
      "Sentiment features: 6\n",
      "TF-IDF features: 1000\n",
      "N-gram features: 100\n",
      "\n",
      "Feature engineering for baseline models complete.\n",
      "Output files are in: hyperpartisan_features_official\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Feature Engineering for Hyperpartisan News Detection (Using Official Splits)\n",
    "# \n",
    "# **Purpose:** Create engineered features suitable for baseline machine learning models, using the official SemEval training and testing datasets. This ensures that vectorizers and scalers are fitted ONLY on the training data.\n",
    "# \n",
    "# **Input:** Reads `official_train_data.csv` and `official_test_data.csv` (generated by `xml_parse.ipynb`).\n",
    "# **Output:** Creates scaled feature matrices (`X_train_scaled.csv`, `X_test_scaled.csv`) and target files (`y_train.csv`, `y_test.csv`) in the `hyperpartisan_features_official` directory.\n",
    "\n",
    "# %%\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize # Use NLTK tokenizer\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# %%\n",
    "# Download required NLTK resources\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True) # Still needed for TF-IDF/N-grams\n",
    "\n",
    "# %%\n",
    "# Initialize sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# %%\n",
    "# --- Configuration ---\n",
    "# Directory where official CSVs are located\n",
    "official_data_dir = \"hyperpartisan_data_official\" \n",
    "# Directory to save engineered features\n",
    "features_dir = \"hyperpartisan_features_official\" # Use a new directory\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "# Input file paths\n",
    "train_csv_path = os.path.join(official_data_dir, \"official_train_data.csv\")\n",
    "test_csv_path = os.path.join(official_data_dir, \"official_test_data.csv\")\n",
    "\n",
    "# Output file paths\n",
    "X_train_output_path = os.path.join(features_dir, 'X_train_scaled.csv')\n",
    "X_test_output_path = os.path.join(features_dir, 'X_test_scaled.csv')\n",
    "y_train_output_path = os.path.join(features_dir, 'y_train.csv')\n",
    "y_test_output_path = os.path.join(features_dir, 'y_test.csv')\n",
    "scaler_path = os.path.join(features_dir, 'scaler.pkl')\n",
    "tfidf_vectorizer_path = os.path.join(features_dir, 'tfidf_vectorizer.pkl')\n",
    "bigram_vectorizer_path = os.path.join(features_dir, 'bigram_vectorizer.pkl')\n",
    "\n",
    "# Feature extraction parameters\n",
    "TFIDF_MAX_FEATURES = 1000\n",
    "NGRAM_MAX_FEATURES = 100\n",
    "\n",
    "# Politically charged terms dictionary (from EDA)\n",
    "politically_charged_terms = {\n",
    "    'trump': 1, 'clinton': 1, 'hillary': 1, 'obama': 1, 'donald': 1, 'biden': 1,\n",
    "    'republican': 1, 'democrat': 1, 'conservative': 1, 'liberal': 1, 'progressive': 1, \n",
    "    'left': 1, 'right': 1, 'leftist': 1, 'rightist': 1, 'gop': 1, 'democratic': 1,\n",
    "    'fake': 1, 'propaganda': 1, 'elite': 1, 'mainstream': 1, 'establishment': 1,\n",
    "    'racist': 1, 'fascist': 1, 'socialist': 1, 'communist': 1, 'radical': 1,\n",
    "    'corruption': 1, 'scandal': 1, 'conspiracy': 1, 'freedom': 1, 'patriot': 1,\n",
    "    'america': 1, 'american': 1, 'nationalism': 1, 'globalist': 1, 'populist': 1,\n",
    "    'lying': 1, 'hoax': 1, 'crooked': 1, 'swamp': 1, 'drain': 1, 'deep state': 1\n",
    "}\n",
    "\n",
    "# Political bigrams list (from EDA)\n",
    "political_bigrams = ['fake news', 'deep state', 'white house', \n",
    "                     'hillary clinton', 'donald trump', 'president trump',\n",
    "                     'white supremacist', 'mainstream media', 'ruling class']\n",
    "\n",
    "# %%\n",
    "# --- Load Data ---\n",
    "print(\"--- Loading Official Train/Test Data ---\")\n",
    "if not os.path.exists(train_csv_path) or not os.path.exists(test_csv_path):\n",
    "    raise FileNotFoundError(f\"Official train/test CSV files not found in '{official_data_dir}'. Please run the XML parsing script first.\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Ensure text columns are strings and handle NaNs\n",
    "train_df['text'] = train_df['text'].fillna(\"\").astype(str)\n",
    "test_df['text'] = test_df['text'].fillna(\"\").astype(str)\n",
    "\n",
    "print(f\"Loaded Training Data: {train_df.shape}\")\n",
    "print(f\"Loaded Test Data: {test_df.shape}\")\n",
    "\n",
    "# Separate target variable\n",
    "y_train = train_df['hyperpartisan'].astype(int)\n",
    "y_test = test_df['hyperpartisan'].astype(int)\n",
    "\n",
    "# %%\n",
    "# --- Feature Extraction Functions (Operating on Raw Text) ---\n",
    "\n",
    "def safe_word_tokenize(text):\n",
    "    \"\"\"Tokenizes text safely, handling potential non-string inputs.\"\"\"\n",
    "    try:\n",
    "        # Ensure input is treated as string, handle potential floats/ints if necessary\n",
    "        return word_tokenize(str(text).lower()) \n",
    "    except Exception: # Catch any unexpected error during tokenization\n",
    "        return []\n",
    "\n",
    "def extract_length_features(df):\n",
    "    \"\"\"Extract features based on article length and structure from RAW text\"\"\"\n",
    "    print(\"Extracting length-based features...\")\n",
    "    features = pd.DataFrame(index=df.index) # Create new df to avoid modifying original\n",
    "    \n",
    "    # Use raw text\n",
    "    raw_text = df['text']\n",
    "    \n",
    "    features['char_count'] = raw_text.apply(len)\n",
    "    features['tokens_raw'] = raw_text.apply(safe_word_tokenize) # Use safe tokenizer\n",
    "    features['word_count_raw'] = features['tokens_raw'].apply(len)\n",
    "    features['unique_word_count_raw'] = features['tokens_raw'].apply(lambda x: len(set(x)))\n",
    "    \n",
    "    features['vocab_diversity_raw'] = features.apply(\n",
    "        lambda x: x['unique_word_count_raw'] / x['word_count_raw'] if x['word_count_raw'] > 0 else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    features['sentence_count'] = raw_text.apply(lambda x: len(sent_tokenize(str(x))) if pd.notna(x) else 0) # Use sent_tokenize\n",
    "    features['avg_sentence_length'] = features.apply(\n",
    "        lambda x: x['word_count_raw'] / x['sentence_count'] if x['sentence_count'] > 0 else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    features['avg_word_length'] = features.apply(\n",
    "        lambda x: x['char_count'] / x['word_count_raw'] if x['word_count_raw'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Drop the intermediate token list\n",
    "    features = features.drop(columns=['tokens_raw']) \n",
    "    \n",
    "    # Paragraph features (less reliable but kept from original)\n",
    "    features['paragraph_count'] = raw_text.apply(lambda x: str(x).count('\\n\\n') + 1)\n",
    "    features['avg_paragraph_length'] = features.apply(\n",
    "        lambda x: x['word_count_raw'] / x['paragraph_count'] if x['paragraph_count'] > 0 else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_lexical_features(df):\n",
    "    \"\"\"Extract features based on presence of politically charged terms in RAW text\"\"\"\n",
    "    print(\"Extracting lexical features...\")\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    raw_text = df['text']\n",
    "\n",
    "    def count_charged_terms(text):\n",
    "        words = safe_word_tokenize(text) # Use safe tokenizer\n",
    "        return sum(1 for word in words if word in politically_charged_terms)\n",
    "\n",
    "    def charged_terms_ratio(text):\n",
    "        words = safe_word_tokenize(text)\n",
    "        if not words: return 0\n",
    "        return count_charged_terms(text) / len(words)\n",
    "\n",
    "    # Specific term counts\n",
    "    for term in ['trump', 'clinton', 'hillary', 'obama', 'republican', 'democrat', \n",
    "                 'conservative', 'liberal', 'fake', 'america', 'american']:\n",
    "        # Use regex for whole word matching, handle potential errors for odd characters\n",
    "        try:\n",
    "            regex = r'\\b' + re.escape(term) + r'\\b'\n",
    "            features[f'count_{term}'] = raw_text.apply(lambda x: len(re.findall(regex, str(x).lower())))\n",
    "        except re.error:\n",
    "            print(f\"Warning: Could not compile regex for term '{term}'. Skipping count.\")\n",
    "            features[f'count_{term}'] = 0\n",
    "\n",
    "\n",
    "    features['political_terms_count'] = raw_text.apply(count_charged_terms)\n",
    "    features['political_terms_ratio'] = raw_text.apply(charged_terms_ratio)\n",
    "\n",
    "    # Political bigram counts\n",
    "    for bigram in political_bigrams:\n",
    "         try:\n",
    "            regex = r'\\b' + re.escape(bigram) + r'\\b'\n",
    "            features[f'count_{bigram.replace(\" \", \"_\")}'] = raw_text.apply(\n",
    "                lambda x: len(re.findall(regex, str(x).lower()))\n",
    "            )\n",
    "         except re.error:\n",
    "             print(f\"Warning: Could not compile regex for bigram '{bigram}'. Skipping count.\")\n",
    "             features[f'count_{bigram.replace(\" \", \"_\")}'] = 0\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_sentiment_features(df):\n",
    "    \"\"\"Extract sentiment-based features from RAW text\"\"\"\n",
    "    print(\"Extracting sentiment features...\")\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    raw_text = df['text']\n",
    "\n",
    "    def get_sentiment(text):\n",
    "        try:\n",
    "            # Ensure text is string for VADER\n",
    "            return sid.polarity_scores(str(text))\n",
    "        except: # Catch potential errors if text is not string-like\n",
    "            return {'compound': 0, 'pos': 0, 'neg': 0, 'neu': 1.0} # Return neutral default\n",
    "\n",
    "    sentiment_scores = raw_text.apply(get_sentiment)\n",
    "    features['sentiment_compound'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "    features['sentiment_positive'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "    features['sentiment_negative'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "    features['sentiment_neutral'] = sentiment_scores.apply(lambda x: x['neu'])\n",
    "\n",
    "    features['sentiment_emotional_ratio'] = features.apply(\n",
    "        lambda x: (x['sentiment_positive'] + x['sentiment_negative']) / x['sentiment_neutral']\n",
    "        if x['sentiment_neutral'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    def sentence_sentiment_variance(text):\n",
    "        try:\n",
    "            sentences = sent_tokenize(str(text))\n",
    "            if len(sentences) <= 1: return 0\n",
    "            sentiments = [sid.polarity_scores(s)['compound'] for s in sentences]\n",
    "            return np.var(sentiments)\n",
    "        except:\n",
    "             return 0 # Return 0 if sentence tokenization fails\n",
    "\n",
    "    features['sentiment_variance'] = raw_text.apply(sentence_sentiment_variance)\n",
    "\n",
    "    return features\n",
    "\n",
    "# %%\n",
    "# --- Feature Generation ---\n",
    "print(\"--- Starting Feature Engineering Process ---\")\n",
    "\n",
    "# Extract non-text features for train and test sets\n",
    "print(\"\\nProcessing Training Set Features...\")\n",
    "train_length_features = extract_length_features(train_df)\n",
    "train_lexical_features = extract_lexical_features(train_df)\n",
    "train_sentiment_features = extract_sentiment_features(train_df)\n",
    "\n",
    "print(\"\\nProcessing Test Set Features...\")\n",
    "test_length_features = extract_length_features(test_df)\n",
    "test_lexical_features = extract_lexical_features(test_df)\n",
    "test_sentiment_features = extract_sentiment_features(test_df)\n",
    "\n",
    "# --- TF-IDF Features ---\n",
    "print(\"\\nExtracting TF-IDF Features (Fit on Train, Transform Train & Test)...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=TFIDF_MAX_FEATURES,\n",
    "    min_df=3, # Keep min_df\n",
    "    max_df=0.95,\n",
    "    stop_words='english'\n",
    ")\n",
    "# Fit ONLY on training data raw text\n",
    "train_tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "# Transform test data raw text\n",
    "test_tfidf_matrix = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Get feature names FROM THE FIT on training data\n",
    "tfidf_feature_names = [f'tfidf_{i}' for i in range(train_tfidf_matrix.shape[1])] # Generic names\n",
    "\n",
    "# Convert sparse matrices to DataFrames with consistent columns\n",
    "X_train_tfidf = pd.DataFrame(train_tfidf_matrix.toarray(), columns=tfidf_feature_names, index=train_df.index)\n",
    "X_test_tfidf = pd.DataFrame(test_tfidf_matrix.toarray(), columns=tfidf_feature_names, index=test_df.index)\n",
    "\n",
    "print(f\"Created {X_train_tfidf.shape[1]} TF-IDF features\")\n",
    "# Save the fitted vectorizer\n",
    "with open(tfidf_vectorizer_path, 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "print(f\"Saved TF-IDF vectorizer to {tfidf_vectorizer_path}\")\n",
    "\n",
    "# --- N-Gram Features (Bigrams) ---\n",
    "print(\"\\nExtracting N-gram Features (Fit on Train, Transform Train & Test)...\")\n",
    "bigram_vectorizer = CountVectorizer(\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=NGRAM_MAX_FEATURES,\n",
    "    min_df=5, # Keep min_df\n",
    "    stop_words='english' # Apply stopwords here as well\n",
    ")\n",
    "# Fit ONLY on training data raw text\n",
    "train_bigram_matrix = bigram_vectorizer.fit_transform(train_df['text'])\n",
    "# Transform test data raw text\n",
    "test_bigram_matrix = bigram_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Get feature names FROM THE FIT on training data\n",
    "bigram_feature_names = [f'bigram_{name.replace(\" \", \"_\")}' for name in bigram_vectorizer.get_feature_names_out()]\n",
    "\n",
    "# Convert sparse matrices to DataFrames\n",
    "X_train_bigram = pd.DataFrame(train_bigram_matrix.toarray(), columns=bigram_feature_names, index=train_df.index)\n",
    "X_test_bigram = pd.DataFrame(test_bigram_matrix.toarray(), columns=bigram_feature_names, index=test_df.index)\n",
    "\n",
    "print(f\"Created {X_train_bigram.shape[1]} bigram features\")\n",
    "# Save the fitted vectorizer\n",
    "with open(bigram_vectorizer_path, 'wb') as f:\n",
    "    pickle.dump(bigram_vectorizer, f)\n",
    "print(f\"Saved Bigram vectorizer to {bigram_vectorizer_path}\")\n",
    "\n",
    "# %%\n",
    "# --- Combine All Features ---\n",
    "print(\"\\nCombining all features...\")\n",
    "\n",
    "X_train_combined = pd.concat([\n",
    "    train_length_features, \n",
    "    train_lexical_features,\n",
    "    train_sentiment_features,\n",
    "    X_train_tfidf,\n",
    "    X_train_bigram\n",
    "], axis=1)\n",
    "\n",
    "X_test_combined = pd.concat([\n",
    "    test_length_features, \n",
    "    test_lexical_features,\n",
    "    test_sentiment_features,\n",
    "    X_test_tfidf,\n",
    "    X_test_bigram\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Final Training Feature Matrix shape: {X_train_combined.shape}\")\n",
    "print(f\"Final Test Feature Matrix shape: {X_test_combined.shape}\")\n",
    "\n",
    "# --- Sanity Check: Ensure columns match exactly ---\n",
    "if not all(X_train_combined.columns == X_test_combined.columns):\n",
    "     print(\"\\nWARNING: Train and Test columns do not match perfectly!\")\n",
    "     # Find differences if any (debugging)\n",
    "     train_cols = set(X_train_combined.columns)\n",
    "     test_cols = set(X_test_combined.columns)\n",
    "     print(\"Columns in Train but not Test:\", sorted(list(train_cols - test_cols)))\n",
    "     print(\"Columns in Test but not Train:\", sorted(list(test_cols - train_cols)))\n",
    "     # Attempt to align columns - adding missing ones with 0s\n",
    "     print(\"Attempting to align columns by adding missing ones with 0...\")\n",
    "     all_cols = X_train_combined.columns.union(X_test_combined.columns)\n",
    "     X_train_combined = X_train_combined.reindex(columns=all_cols, fill_value=0)\n",
    "     X_test_combined = X_test_combined.reindex(columns=all_cols, fill_value=0)\n",
    "     print(f\"Aligned Training Feature Matrix shape: {X_train_combined.shape}\")\n",
    "     print(f\"Aligned Test Feature Matrix shape: {X_test_combined.shape}\")\n",
    "\n",
    "# %%\n",
    "# --- Scale Numerical Features ---\n",
    "print(\"\\nScaling numerical features...\")\n",
    "# Identify numerical columns (excluding potentially introduced object columns if errors occurred)\n",
    "numeric_cols = X_train_combined.select_dtypes(include=np.number).columns.tolist()\n",
    "print(f\"Found {len(numeric_cols)} numerical features to scale.\")\n",
    "\n",
    "# Initialize and fit scaler ONLY on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train_combined.copy()\n",
    "X_test_scaled = X_test_combined.copy()\n",
    "\n",
    "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train_combined[numeric_cols])\n",
    "# Transform test data using the scaler fitted on train data\n",
    "X_test_scaled[numeric_cols] = scaler.transform(X_test_combined[numeric_cols])\n",
    "print(\"Features scaled.\")\n",
    "\n",
    "# --- Save Processed Data ---\n",
    "print(\"\\nSaving scaled features and labels...\")\n",
    "# Save the fitted scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Saved Scaler to {scaler_path}\")\n",
    "\n",
    "# Save feature matrices and labels\n",
    "X_train_scaled.to_csv(X_train_output_path, index=False)\n",
    "X_test_scaled.to_csv(X_test_output_path, index=False)\n",
    "y_train.to_csv(y_train_output_path, index=False, header=True) # Include header for Series\n",
    "y_test.to_csv(y_test_output_path, index=False, header=True) # Include header for Series\n",
    "print(f\"Saved training features to {X_train_output_path}\")\n",
    "print(f\"Saved test features to {X_test_output_path}\")\n",
    "print(f\"Saved training labels to {y_train_output_path}\")\n",
    "print(f\"Saved test labels to {y_test_output_path}\")\n",
    "\n",
    "# Save a sample of scaled features for inspection\n",
    "X_train_scaled.head(10).to_csv(os.path.join(features_dir, 'X_train_scaled_sample.csv'), index=False)\n",
    "\n",
    "# --- Final Feature Summary ---\n",
    "print(\"\\n--- Feature Engineering Summary ---\")\n",
    "print(f\"Total features generated: {X_train_scaled.shape[1]}\")\n",
    "# Recalculate feature type counts based on final columns\n",
    "final_cols = X_train_scaled.columns\n",
    "length_features_final = [col for col in final_cols if col in train_length_features.columns]\n",
    "lexical_features_final = [col for col in final_cols if col in train_lexical_features.columns]\n",
    "sentiment_features_final = [col for col in final_cols if col in train_sentiment_features.columns]\n",
    "tfidf_features_final = [col for col in final_cols if col.startswith('tfidf_')]\n",
    "ngram_features_final = [col for col in final_cols if col.startswith('bigram_')]\n",
    "\n",
    "print(f\"Length features: {len(length_features_final)}\")\n",
    "print(f\"Lexical features: {len(lexical_features_final)}\")\n",
    "print(f\"Sentiment features: {len(sentiment_features_final)}\")\n",
    "print(f\"TF-IDF features: {len(tfidf_features_final)}\")\n",
    "print(f\"N-gram features: {len(ngram_features_final)}\")\n",
    "\n",
    "print(\"\\nFeature engineering for baseline models complete.\")\n",
    "print(f\"Output files are in: {features_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-CPv1Fx27-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
