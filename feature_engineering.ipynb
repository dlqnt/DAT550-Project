{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Hyperpartisan News Detection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load preprocessed data\n",
    "data_dir = \"hyperpartisan_data\"\n",
    "articles_df = pd.read_csv(f\"{data_dir}/articles_preprocessed.csv\")\n",
    "print(f\"Loaded {len(articles_df)} articles\")\n",
    "\n",
    "# Create a directory for feature outputs\n",
    "features_dir = \"hyperpartisan_features\"\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "# Initialize a dictionary to hold politically charged terms\n",
    "# Based on our EDA findings\n",
    "politically_charged_terms = {\n",
    "    # Political figures\n",
    "    'trump': 1, 'clinton': 1, 'hillary': 1, 'obama': 1, 'donald': 1, 'biden': 1,\n",
    "    \n",
    "    # Political parties/ideologies\n",
    "    'republican': 1, 'democrat': 1, 'conservative': 1, 'liberal': 1, 'progressive': 1, \n",
    "    'left': 1, 'right': 1, 'leftist': 1, 'rightist': 1, 'gop': 1, 'democratic': 1,\n",
    "    \n",
    "    # Charged political terms\n",
    "    'fake': 1, 'propaganda': 1, 'elite': 1, 'mainstream': 1, 'establishment': 1,\n",
    "    'racist': 1, 'fascist': 1, 'socialist': 1, 'communist': 1, 'radical': 1,\n",
    "    'corruption': 1, 'scandal': 1, 'conspiracy': 1, 'freedom': 1, 'patriot': 1,\n",
    "    'america': 1, 'american': 1, 'nationalism': 1, 'globalist': 1, 'populist': 1,\n",
    "    'lying': 1, 'hoax': 1, 'crooked': 1, 'swamp': 1, 'drain': 1, 'deep state': 1\n",
    "}\n",
    "\n",
    "# 1. ARTICLE LENGTH FEATURES\n",
    "def extract_length_features(df):\n",
    "    \"\"\"Extract features based on article length and structure\"\"\"\n",
    "    print(\"Extracting length-based features...\")\n",
    "    \n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic length features\n",
    "    df['word_count'] = df['text'].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "    df['char_count'] = df['text'].fillna(\"\").apply(len)\n",
    "    df['avg_word_length'] = df.apply(\n",
    "        lambda x: x['char_count'] / x['word_count'] if x['word_count'] > 0 else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Processed text features\n",
    "    df['processed_word_count'] = df['processed_text'].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "    df['unique_word_count'] = df['processed_text'].fillna(\"\").apply(lambda x: len(set(x.split())))\n",
    "    \n",
    "    # Vocabulary diversity\n",
    "    df['vocab_diversity'] = df.apply(\n",
    "        lambda x: x['unique_word_count'] / x['processed_word_count'] if x['processed_word_count'] > 0 else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Sentence structure\n",
    "    df['sentence_count'] = df['text'].fillna(\"\").apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "    df['avg_sentence_length'] = df.apply(\n",
    "        lambda x: x['word_count'] / x['sentence_count'] if x['sentence_count'] > 0 else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Paragraph structure (approximated by double newlines)\n",
    "    df['paragraph_count'] = df['text'].fillna(\"\").apply(lambda x: x.count('\\n\\n') + 1)\n",
    "    df['avg_paragraph_length'] = df.apply(\n",
    "        lambda x: x['word_count'] / x['paragraph_count'] if x['paragraph_count'] > 0 else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create length-based feature subset\n",
    "    length_features = [\n",
    "        'word_count', 'char_count', 'avg_word_length', \n",
    "        'unique_word_count', 'vocab_diversity',\n",
    "        'sentence_count', 'avg_sentence_length',\n",
    "        'paragraph_count', 'avg_paragraph_length'\n",
    "    ]\n",
    "    \n",
    "    return df[length_features]\n",
    "\n",
    "# 2. LEXICAL FEATURES - POLITICALLY CHARGED TERMS\n",
    "def extract_lexical_features(df):\n",
    "    \"\"\"Extract features based on presence of politically charged terms\"\"\"\n",
    "    print(\"Extracting lexical features...\")\n",
    "    \n",
    "    # Create a copy\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Function to count politically charged terms\n",
    "    def count_charged_terms(text):\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return 0\n",
    "        words = word_tokenize(text.lower())\n",
    "        return sum(1 for word in words if word in politically_charged_terms)\n",
    "    \n",
    "    # Function to calculate ratio of charged terms\n",
    "    def charged_terms_ratio(text):\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return 0\n",
    "        words = word_tokenize(text.lower())\n",
    "        if len(words) == 0:\n",
    "            return 0\n",
    "        return count_charged_terms(text) / len(words)\n",
    "    \n",
    "    # Extract counts for specific political terms\n",
    "    for term in ['trump', 'clinton', 'hillary', 'obama', 'republican', 'democrat', \n",
    "                 'conservative', 'liberal', 'fake', 'america', 'american']:\n",
    "        df[f'count_{term}'] = df['text'].fillna(\"\").apply(\n",
    "            lambda x: len(re.findall(r'\\b' + term + r'\\b', x.lower()))\n",
    "        )\n",
    "    \n",
    "    # Create aggregate features\n",
    "    df['political_terms_count'] = df['text'].fillna(\"\").apply(count_charged_terms)\n",
    "    df['political_terms_ratio'] = df['text'].fillna(\"\").apply(charged_terms_ratio)\n",
    "    \n",
    "    # Count political bigrams\n",
    "    political_bigrams = ['fake news', 'deep state', 'white house', \n",
    "                         'hillary clinton', 'donald trump', 'president trump',\n",
    "                         'white supremacist', 'mainstream media', 'ruling class']\n",
    "    \n",
    "    for bigram in political_bigrams:\n",
    "        df[f'count_{bigram.replace(\" \", \"_\")}'] = df['text'].fillna(\"\").apply(\n",
    "            lambda x: len(re.findall(r'\\b' + bigram + r'\\b', x.lower()))\n",
    "        )\n",
    "    \n",
    "    # Select lexical features\n",
    "    lexical_features = [col for col in df.columns if col.startswith('count_') or col.endswith('_ratio')]\n",
    "    \n",
    "    return df[lexical_features]\n",
    "\n",
    "# 3. SENTIMENT FEATURES\n",
    "def extract_sentiment_features(df):\n",
    "    \"\"\"Extract sentiment-based features\"\"\"\n",
    "    print(\"Extracting sentiment features...\")\n",
    "    \n",
    "    # Create a copy\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Function to extract VADER sentiment\n",
    "    def get_sentiment(text):\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return {'compound': 0, 'pos': 0, 'neg': 0, 'neu': 0}\n",
    "        return sid.polarity_scores(text)\n",
    "    \n",
    "    # Apply sentiment analysis\n",
    "    df['sentiment'] = df['text'].fillna(\"\").apply(get_sentiment)\n",
    "    df['sentiment_compound'] = df['sentiment'].apply(lambda x: x['compound'])\n",
    "    df['sentiment_positive'] = df['sentiment'].apply(lambda x: x['pos'])\n",
    "    df['sentiment_negative'] = df['sentiment'].apply(lambda x: x['neg'])\n",
    "    df['sentiment_neutral'] = df['sentiment'].apply(lambda x: x['neu'])\n",
    "    \n",
    "    # Calculate derived sentiment features\n",
    "    df['sentiment_emotional_ratio'] = df.apply(\n",
    "        lambda x: (x['sentiment_positive'] + x['sentiment_negative']) / x['sentiment_neutral'] \n",
    "        if x['sentiment_neutral'] > 0 else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate sentiment variance by sentence\n",
    "    def sentence_sentiment_variance(text):\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return 0\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        if len(sentences) <= 1:\n",
    "            return 0\n",
    "        sentiments = [sid.polarity_scores(s)['compound'] for s in sentences]\n",
    "        return np.var(sentiments)\n",
    "    \n",
    "    df['sentiment_variance'] = df['text'].fillna(\"\").apply(sentence_sentiment_variance)\n",
    "    \n",
    "    # Select sentiment features\n",
    "    sentiment_features = [\n",
    "        'sentiment_compound', 'sentiment_positive', 'sentiment_negative', \n",
    "        'sentiment_neutral', 'sentiment_emotional_ratio', 'sentiment_variance'\n",
    "    ]\n",
    "    \n",
    "    return df[sentiment_features]\n",
    "\n",
    "# 4. TF-IDF FEATURES\n",
    "def extract_tfidf_features(df, max_features=1000):\n",
    "    \"\"\"Extract TF-IDF features from text\"\"\"\n",
    "    print(\"Extracting TF-IDF features...\")\n",
    "    \n",
    "    # Create a TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        min_df=3,\n",
    "        max_df=0.95,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the processed text\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'].fillna(\"\"))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    tfidf_df = pd.DataFrame(\n",
    "        tfidf_matrix.toarray(),\n",
    "        columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Save the vectorizer for future use\n",
    "    with open(os.path.join(features_dir, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "    print(f\"Created {tfidf_matrix.shape[1]} TF-IDF features\")\n",
    "    return tfidf_df\n",
    "\n",
    "# 5. N-GRAM FEATURES\n",
    "def extract_ngram_features(df, max_features=100):\n",
    "    \"\"\"Extract important n-gram features\"\"\"\n",
    "    print(\"Extracting n-gram features...\")\n",
    "    \n",
    "    # Create bigram vectorizer\n",
    "    bigram_vectorizer = CountVectorizer(\n",
    "        ngram_range=(2, 2),\n",
    "        max_features=max_features,\n",
    "        min_df=5\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the processed text\n",
    "    bigram_matrix = bigram_vectorizer.fit_transform(df['processed_text'].fillna(\"\"))\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = bigram_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    bigram_df = pd.DataFrame(\n",
    "        bigram_matrix.toarray(),\n",
    "        columns=[f'bigram_{name.replace(\" \", \"_\")}' for name in feature_names]\n",
    "    )\n",
    "    \n",
    "    # Save the vectorizer for future use\n",
    "    with open(os.path.join(features_dir, 'bigram_vectorizer.pkl'), 'wb') as f:\n",
    "        pickle.dump(bigram_vectorizer, f)\n",
    "    \n",
    "    print(f\"Created {len(feature_names)} bigram features\")\n",
    "    return bigram_df\n",
    "\n",
    "# 6. COMBINED FEATURE EXTRACTION\n",
    "def create_feature_matrix(df, include_tfidf=True, include_ngrams=True):\n",
    "    \"\"\"Combine all features into a single feature matrix\"\"\"\n",
    "    print(\"Creating combined feature matrix...\")\n",
    "    \n",
    "    # Extract different feature sets\n",
    "    length_features_df = extract_length_features(df)\n",
    "    lexical_features_df = extract_lexical_features(df)\n",
    "    sentiment_features_df = extract_sentiment_features(df)\n",
    "    \n",
    "    # Combine base features\n",
    "    combined_features = pd.concat([\n",
    "        length_features_df, \n",
    "        lexical_features_df,\n",
    "        sentiment_features_df\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Add TF-IDF features if requested (can be large)\n",
    "    if include_tfidf:\n",
    "        tfidf_features_df = extract_tfidf_features(df)\n",
    "        combined_features = pd.concat([combined_features, tfidf_features_df], axis=1)\n",
    "    \n",
    "    # Add n-gram features if requested\n",
    "    if include_ngrams:\n",
    "        ngram_features_df = extract_ngram_features(df)\n",
    "        combined_features = pd.concat([combined_features, ngram_features_df], axis=1)\n",
    "    \n",
    "    print(f\"Created combined feature matrix with {combined_features.shape[1]} features\")\n",
    "    return combined_features\n",
    "\n",
    "# MAIN EXECUTION\n",
    "# Step 1: Create combined feature set\n",
    "print(\"Starting feature engineering process...\")\n",
    "features_df = create_feature_matrix(articles_df)\n",
    "\n",
    "# Step 2: Add target variable\n",
    "features_df['hyperpartisan'] = articles_df['hyperpartisan']\n",
    "\n",
    "# Step 3: Split into training and testing sets\n",
    "X = features_df.drop('hyperpartisan', axis=1)\n",
    "y = features_df['hyperpartisan']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 4: Scale numerical features\n",
    "# Get only numeric columns\n",
    "numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Step 5: Save processed data\n",
    "with open(os.path.join(features_dir, 'scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "X_train.to_csv(os.path.join(features_dir, 'X_train.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(features_dir, 'X_test.csv'), index=False)\n",
    "y_train.to_csv(os.path.join(features_dir, 'y_train.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(features_dir, 'y_test.csv'), index=False)\n",
    "\n",
    "# Save a sample of features for inspection\n",
    "features_df.head(10).to_csv(os.path.join(features_dir, 'features_sample.csv'), index=False)\n",
    "\n",
    "# Print feature categories counts\n",
    "length_features = [col for col in features_df.columns if col in extract_length_features(articles_df).columns]\n",
    "lexical_features = [col for col in features_df.columns if col in extract_lexical_features(articles_df).columns]\n",
    "sentiment_features = [col for col in features_df.columns if col in extract_sentiment_features(articles_df).columns]\n",
    "tfidf_features = [col for col in features_df.columns if col.startswith('tfidf_')]\n",
    "ngram_features = [col for col in features_df.columns if col.startswith('bigram_')]\n",
    "\n",
    "print(\"\\nFeature Summary:\")\n",
    "print(f\"Total features: {len(features_df.columns) - 1}\")  # Exclude target\n",
    "print(f\"Length features: {len(length_features)}\")\n",
    "print(f\"Lexical features: {len(lexical_features)}\")\n",
    "print(f\"Sentiment features: {len(sentiment_features)}\")\n",
    "print(f\"TF-IDF features: {len(tfidf_features)}\")\n",
    "print(f\"N-gram features: {len(ngram_features)}\")\n",
    "\n",
    "print(\"\\nFeature engineering complete. Files saved to:\", features_dir)\n",
    "print(\"Now you can proceed to model training with these engineered features.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
