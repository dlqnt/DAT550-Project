{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SemEval datasets...\n",
      "Downloaded by-article dataset to hyperpartisan_data/articles-training-byarticle.zip\n",
      "Extracted by-article dataset\n",
      "Skipping by-publisher dataset due to its large size (754k articles)\n",
      "XML file not found at hyperpartisan_data/articles-training-byarticle/articles.xml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "# Create data directory\n",
    "data_dir = \"hyperpartisan_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "def download_semeval_dataset():\n",
    "    \"\"\"\n",
    "    Download the SemEval-2019 Task 4 hyperpartisan news detection datasets\n",
    "    \"\"\"\n",
    "    print(\"Downloading SemEval datasets...\")\n",
    "    \n",
    "    # URLs for the datasets\n",
    "    by_article_url = \"https://zenodo.org/records/1489920/files/articles-training-byarticle-20181122.zip\"\n",
    "    \n",
    "    # Download by-article dataset (smaller, manually labeled)\n",
    "    response = requests.get(by_article_url)\n",
    "    if response.status_code == 200:\n",
    "        by_article_path = os.path.join(data_dir, \"articles-training-byarticle.zip\")\n",
    "        with open(by_article_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded by-article dataset to {by_article_path}\")\n",
    "        \n",
    "        # Extract the ZIP file\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(by_article_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"Extracted by-article dataset\")\n",
    "    else:\n",
    "        print(f\"Failed to download by-article dataset: Status code {response.status_code}\")\n",
    "        \n",
    "    # Note: The by-publisher dataset is very large (754k articles)\n",
    "    # For initial development, we'll use only the by-article dataset\n",
    "    print(\"Skipping by-publisher dataset due to its large size (754k articles)\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = \"hyperpartisan_data\"\n",
    "\n",
    "# Check for the actual XML file\n",
    "possible_xml_paths = [\n",
    "    os.path.join(data_dir, \"articles-training-byarticle\", \"articles.xml\"),\n",
    "    os.path.join(data_dir, \"articles-training-byarticle-20181122.xml\"),\n",
    "    os.path.join(data_dir, \"articles.xml\")\n",
    "]\n",
    "\n",
    "xml_file = None\n",
    "for path in possible_xml_paths:\n",
    "    if os.path.exists(path):\n",
    "        xml_file = path\n",
    "        print(f\"Found XML file at: {path}\")\n",
    "        break\n",
    "\n",
    "if xml_file:\n",
    "    print(f\"Parsing XML file: {xml_file}\")\n",
    "    \n",
    "    # Parse the XML file\n",
    "    def parse_xml_to_dataframe(xml_file):\n",
    "        \"\"\"Parse the XML file into a pandas DataFrame\"\"\"\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        articles = []\n",
    "        for article in root.findall('.//article'):\n",
    "            # Extract article attributes\n",
    "            article_id = article.get('id')\n",
    "            title = article.get('title')\n",
    "            published_at = article.get('published-at')\n",
    "            hyperpartisan = article.get('hyperpartisan')\n",
    "            \n",
    "            # Extract article text\n",
    "            paragraphs = article.findall('.//p')\n",
    "            text = '\\n'.join([p.text if p.text else '' for p in paragraphs])\n",
    "            \n",
    "            articles.append({\n",
    "                'id': article_id,\n",
    "                'title': title,\n",
    "                'published_at': published_at,\n",
    "                'hyperpartisan': hyperpartisan == 'true',  # Convert to boolean\n",
    "                'text': text\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(articles)\n",
    "    \n",
    "    # Parse the XML and save as CSV\n",
    "    articles_df = parse_xml_to_dataframe(xml_file)\n",
    "    csv_path = os.path.join(data_dir, \"articles_byarticle.csv\")\n",
    "    articles_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"Parsed {len(articles_df)} articles and saved to {csv_path}\")\n",
    "    print(f\"Label distribution: {articles_df['hyperpartisan'].value_counts()}\")\n",
    "    \n",
    "    # Show a sample\n",
    "    print(\"\\nSample article:\")\n",
    "    print(articles_df.iloc[0][['title', 'hyperpartisan']])\n",
    "    print(articles_df.iloc[0]['text'][:200] + \"...\")\n",
    "else:\n",
    "    print(\"No XML file found. Please check the extracted contents.\")\n",
    "    print(\"Current files in the data directory:\")\n",
    "    for file in os.listdir(data_dir):\n",
    "        print(f\"- {file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-CPv1Fx27-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
